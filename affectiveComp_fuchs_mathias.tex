
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/
%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}
\usepackage{amssymb}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.




\usepackage{xcolor}
\newcommand\note[1]{\textcolor{red}{#1}}


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
\usepackage{float}
\usepackage{hyperref}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.


\usepackage{graphicx}
\graphicspath{ {images/} }


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Affective Computing For Empathic Behaviour Change}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Mathias Fuchs}
\IEEEauthorblockA{University of Bern \\
Master of computer science\\
Matr. Nr.: 09-923-764\\
Email: fuchsmat@students.unibe.ch}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
Humans strive to build machines that can interact with humans in a humanoid way. This is why it is crucial for a computer to be able to understand in which emotional state the user is in. To achieve such a feat there are different approaches.
Within the research area of affective computing, a large part of the studies focusses on facial expressions and changes in speech. These expressions are good to recognize the emotional state of a human during social interaction, however they may not be suitable  in other situations for example recognizing emotions from a greater distance\cite{karg2012pattern}.
In this paper I give an overview over the body language recognition approaches done today and propose a model which analyses emotions based on the way a human subject walks.

\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the conference you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals/conferences frown on
% math in the abstract anyway.

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
For a long time people were convinced that human behaviour is "all nurture and no nature"\cite{eckman2003emotions}. However already Darwin\cite{darwin} suggested that along with the facial expression, the human body movements and the gestures also represent the state of mind and the corresponding emotions of humans. We know today that body language pays a very important contribution to understand the affective state of a person\cite{ekman1967head,mccoll2012affect}. Surprisingly only 7\% of human communication are made of words and 55\% are made up of non-verbal communication\cite{eckman2003emotions,singhsignificance,mehrabian1969significance}. 
The idea of this paper was to focus on micro expressions in body language. However there are no bodily micro expressions as in facial micro expressions. A micro expression is a "very fast facial movement lasting less than one-fifth of a second"\cite{eckman2003emotions}. Body language in comparison can be subconscious however it can be consciously changed more easily, than a facial micro expression. This is the reason that I focus on the emotion detection via body language in this paper.

There are various uses for emotion detection by body language. Some of those are detecting the \emph{affective state} of a person, \emph{lie detection}, the degree of \emph{accessibility} towards another person etc. Different indicators for interpreting body language can be \emph{body position and distance}\cite{mccoll2012affect,mehrabian1969significance}, \emph{body movement}\cite{singhsignificance,karg2012pattern} and \emph{hand form}\cite{wang2007hand,triesch2001system}. This list is not necessarily concluding, but those are the parts that I focussed on in this work.

\begin{figure}[H]
\centering
   \includegraphics[width=\linewidth]{phasesOfEmotionDetectingSystem.jpg}
  \caption{The phases of a bodily emotion detection system\cite{singhsignificance}}
  \label{fig:phasesOfEmotionDetectingSystem}
\end{figure}


In the field of affective computing however, it is not only a challenge to interpret the body language, but also to detect the position of the human posture and get useful data out of it. This leads to the two main challenges: \emph{1. Detection of the posture} and \emph{2. How to interpret the posture representation} ( see \autoref{fig:phasesOfEmotionDetectingSystem}\cite{singhsignificance}). However those two steps are done in one with certain machine learning approaches that consist of neural networks\cite{schindler2008recognizing, kleinsmith2007recognizing, kleinsmith2005incremental}.

In the following work I will first give an overview over the psychology behind emotion detection through body language. After that I present papers that provide approaches for both, the detection of the human posture as well as for interpreting the found results. Finally I propose a theoretical model of how to recognize human emotion by gait in real-time. 

\section{Emotions through body language}
\label{sec:emotions}
As already stated, 55\% of our communication consists of non-verbal cues, like body language. The expression of emotions has been studied extensively\cite{tomkins1962,ekmann1973universal,ekman1993facial,frijda1986emotions}. According to Eckman\cite{eckman2003emotions} there are 6 basic categories of emotions:
\begin{itemize}
\item{Anger}
\item{Disgust}
\item{Fear}
\item{Happiness}
\item{Sadness}
\item{Surprise}
\end{itemize}

\begin{figure}
\centering
   \includegraphics[width=\linewidth]{basicEmotions.jpg}
   \caption{Facial expression of the six basic emotions ( and contempt)}
  \label{fig:basicEmotions}
\end{figure}

Those emotions seem to be universal and the same across different cultures\cite{ekmann1973universal}. The motivation for those 6 categories goes back to the theory, that the same facial muscles are used for the same emotions across different cultures\cite{schindler2008recognizing}. This categorical notion of emotions allows systems to more easily categorize new samples to one of those existing categories, instead of having to create new categories from scratch\cite{schindler2008recognizing}.
We can find clear signs of all those emotions in our faces (see \autoref{fig:basicEmotions}\footnote{\url{https://hubpages.com/health/Facial-Expressions-Emotions-and-Feelings}}), because those signs are mostly shown involuntary micro expressions. Recently more research has been done in the field of detecting emotions through body language, like body movement and body pose\cite{de2006towards,grezes2007perceiving,meeren2005rapid}. In \autoref{fig:basicEmotionsBodyLanguage} we can see a representation of the body pose for the 6 basic emotions. 
\begin{figure}
\centering
   \includegraphics[width=\linewidth]{basicEmotionsBodylanguage.jpg}
  \caption{Bodily signs of the six basic emotions\cite{schindler2008recognizing}}
  \label{fig:basicEmotionsBodyLanguage}
\end{figure}

In \autoref{fig:bodyPostureTable} we can see a possible way on how to interpret certain bodily signs based on the body position and body movement. 
\begin{figure}
\centering
   \includegraphics[width=\linewidth]{bodyPostureTable.jpg}
  \caption{A table of bodily signs for different emotions\cite{singhsignificance}}
  \label{fig:bodyPostureTable}
\end{figure}
The distance between two people can also show the attitude between them. If the distance is less than the local social norms permit, we can infer a negative attitude\cite{mehrabian1969significance}. However this has to be treated carefully, because the context is important. For example, if two people that are in a romantic relationship with each other, then less distance is not a negative sign. It has also been found, that the meaning of distance depends on the expectancies of the subjects. If the subjects expect a positive interaction, then a close distance is a positive thing, and if they expect a negative interaction, then a close distance is negative\cite{mehrabian1969significance}. Examples of approaches in \autoref{sec:emotionBodyMovementPos}.\

In addition larger body movements like the gait of a person, can also tell us something about the emotional state of a person. It has been found that the arm swing, the stire length and the heavy-footedness of a persons gait, can reveaal their emotional state\cite{montepare1987identification}. For example are angry movements larger, faster and seem jerky, compared to normal movements\cite{singhsignificance}. Whereas fearful and sad movements tend to be less energetic, smaller and slower\cite{singhsignificance}. Examples of approaches in \autoref{sec:emotionGait}.\


The form of the hand can also give information about the emotional state of a person. For example open palms could mean pleasure/ openness, closed hands towards the chest could mean a sense of pride and clenched fists could mean anger\cite{beale2008affect,singh2015edbl}. Kipp and Martin (2009) et al.\cite{kipp2009gesture} even stated, that for right-handed people, the right hand was more used when experiencing anger and the left hand more, when their feelings were relaxed and positive. Examples of approaches in \autoref{sec:emotionHand}.

In the following sections I summarize a few approaches that extract and/or interpret body language in different ways. 


\section{Emotion detection through body position}
\label{sec:emotionBodyMovementPos}
\subsection{Lie Detection based on Facial Micro Expression, body Language and Speech Analysis}
\label{subsec:lieDetectionLAM}
A very interesting approach was done by Barathi\cite{barathi2016lie}. 
\subsubsection{Body pose extraction}
To extract the body poses from videos the Limb Action Model Converter\cite{du20143d} has been used. This converter uses Microsoft Kinect as a base. 
The Limb Action Model extracts 10 limbs from a body posture: "Spine to center shoulder, center shoulder to head, left/right shoulder to left/right elbow, left/right elbow to left/right wrist, left/right hip to left/right knee and left/right knee to left/right ankle"\cite{du20143d}. In \autoref{fig:limbAngleModel} we can see how the posture is represented after the extraction.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{limbAngleModel.jpg}
\caption{Skeleton joints extracted with the Limb Angle Model\cite{du20143d}}
\label{fig:limbAngleModel}
\end{figure}

\subsubsection{Interpretation}
As the paper focusses on lie detection, they defined the following signs for lying\cite{kinsey2012,sorjoo,barathi2016lie}:
\begin{itemize}
\item{Increasing hand to face/mouth gestures}
\item{Nose touching: Because of an adrenaline rush, the capillaries open up, which causes the nose to itch}
\item{Place the hand close to or over the mouth}
\item{Small gestures like lip biting, hands rubbing, fidgeting}
\item{Clenched fist, crossed arms}
\end{itemize}
According to those specifications they trained their system with images of liars that were exhibiting those typical body language cues for lying and pictures of people who were not lying. All the images are subjected to the Limb Action Model Converter. Finally they clustered the converted pictures with k-means.
Sadly they did not provide an evaluation for the body language part of their method.

\subsection{EDBL - Algorithm for Detection and Analysis of Emotion Using Body Language }
\subsubsection{Body pose extraction}
EDBL\cite{singh2015edbl} relies on a pose estimation which is using postelets for humnan parsing\cite{wang2011learning}. Different postelets are extracted from images. A linear SVM classifier is trained for detecting the presence of each postelet. In the end we get a complete model of the human. See \autoref{fig:posteletsFeatureExtraction} for a graphical representation of the process.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{posteletsFeatureExtraction.jpg}
\caption{Graphical illustration of the postelet model\cite{wang2011learning}}
\label{fig:posteletsFeatureExtraction}
\end{figure}

Based on the postelet representation a line graph (see \autoref{fig:edblStickPose} of the extracted pose is created and used for the interpretation of the body posture. 

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{edblStickPose.jpg}
\caption{EDBL stick pose representantion\cite{singh2015edbl}}
\label{fig:edblStickPose}
\end{figure}

\subsubsection{Interpretation}
To figure out the emotional state of a person, the position of the shoulders is interpreted. There is differentiated between three different types of poses: 1. normal/ calm, 2. confused, amazed or in doubt and 3. depressed/ not interested. \autoref{fig:shoulderPositionAndMeaning} shows the three different types. First the slope of the normal shoulder position $\theta$ is calculated. An initialisation pose is needed. Then it is compared to other images of the same person and if the slope is bigger than the slope of 	$\theta$ then it is a sign of doubt, confusion or being amazed. If the slope is smaller than the slope of $\theta$ then it is a sign of a depressed, not-interested or lazy emotional state.
 
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{shoulderPositionAndMeaning.jpg}
\caption{Meaning of shoulder positions. (a) shows a person in normal or calm position, (b) shows a person in confused or amazed state and (c) shows a person in a depressed or not interested pose.\cite{singh2015edbl}}
\label{fig:shoulderPositionAndMeaning}
\end{figure}

\section{Affect Detection from Body Language during Social HRI}
\subsubsection{Body pose extraction}
This approach of McColl et al\cite{mccoll2012affect} also uses Microsoft Kinect as a base to extract the body pose. It creates an ellipsoid model as seen in \autoref{fig:ellipsoidModel}. To do this, the Kinect sensor first performs a human body extraction. Then the extracted body poses are observed to see if a static pose is displayed. Once a static pose is identified the segmented body parts are fitted with ellipsoids\cite{mccoll2012affect}. The static body poses are further explained in \autoref{subsubsec:ellipsoidModelInterpretation}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{ellipsoidModel.jpg}
\caption{An ellipsoid model od the human body built on the basis of Microsoft Kinect.\cite{singh2015edbl}}
\label{fig:ellipsoidModel}
\end{figure}

\subsubsection{Interpretation} 
\label{subsubsec:ellipsoidModelInterpretation}
To figure out the emotional state of the subject, static body poses are used. The poses are based on the Nonverbal Interaction States Analysis of the Davis Nonverbal States Scale\cite{davis1994nonverbal}. Body angle, trunk lean and arm position are evaluated. The resulting metrics are the following: 
\begin{itemize}
\item{\textbf{Three different body angles:} \emph{Toward(T)}: 0°-3° angle from the robot, \emph{Neutral(N)}: 3°-15° angle from the robot, \emph{Away(A)}: >15° from the robot}
\item{\textbf{Trunk lean:}\emph{Upright}: The shoulders are over the hips, \emph{Forward/ backward lean}: The shoulders are closer/ farther away than the hips in relation to the robot, \emph{Right/ left lean}: The right/ left shoulder is tilted past the right/ left hip.}
\item{\textbf{Arm positions:} \emph{T}: The arms are closer to the robot than the upper trunk, \emph{A}: The arms are farther from the robot than the trunk, \emph{N}: else}
\end{itemize}

According to those metrics 4 accessibility levels\cite{mccoll2012affect} have been defined (see \autoref{fig:accessibilityLevels}). Level I-IV, where I is least accessible and IV is the most accessible state. The arm orientation is used for a finer scaling of the accessibility levels.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{accessibilityLevels.jpg}
\caption{Different accessibility levels defined to determine the level of accessibility of the person against the system\cite{mccoll2012affect}}
\label{fig:accessibilityLevels}
\end{figure}


This approach classified acessibility with an accuracy of 88\%\cite{mccoll2012affect}.


\subsection{Recognizing Emotions Expressed by Body Pose: a Biologically Inspired Neural Model}
This approach by Schindler et al.\cite{schindler2008recognizing} aims to categorize images into the 6 basic emotions\cite{eckman2003emotions} \emph{angry, disgusted, fearful, happy, sad, surprised and neutral}. The Idea is to model the visual pathway of recognition of a human. This means the process in which the visual cortex extracts cues from the visual input to determine an emotion\cite{schindler2008recognizing}. The model is inspired by \cite{riesenhuber1999hierarchical,serre2007robust}. This approach is interesting, because it combines various commonly used and efficient techniques like MAX pooling, PCA and SVM in a hierarchical way that follows the organization of the human visual cortex.

The visual cortex contains various areas: \emph{Primary visual cortex (V1), secondary visual cortex or prestriate cortex (V2), visual area (V3), visual area (V4) and middle temporal visual area (V5)}\footnote{\url{https://en.wikipedia.org/wiki/Visual_cortex}}. The model is built according to those areas (see \autoref{fig:biologicallyInspiredNeuralModel}). 
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{biologicallyInspiredNeuralModel.jpg}
\caption{"Illustration of the neural model. From the raw image on the retina, local orientation is extracted (area V1), pooled over spatial neighborhoods (V2/V4), and filtered with learned complex features (V4/I).T The filter response serves as input
into a discriminative classifier (IT). Parameters were chosen for illustration purposes and are different from the actual implementation."\cite{schindler2008recognizing}}
\label{fig:biologicallyInspiredNeuralModel}
\end{figure}

The recall of the system achieved 82\% whereas the recall for human testers was 87\%. This means that on average the model only miss-classified 5\% more than the human testers. Interesting to see is the direct comparison against the human for the single emotions that were classified (see \autoref{fig:biologicallyInspiredNeuralModelEval}).

This approach is quite interesting, because it performs fairly well, even though the training set only consists of 50 actors that created a total of 696 images.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{biologicallyInspiredNeuralModelEval.jpg}
\caption{The classification performance of the model in comparison to the human testers\cite{schindler2008recognizing}}
\label{fig:biologicallyInspiredNeuralModelEval}
\end{figure}





\subsection{Real Time Multi-Person 2D Pose Estimation using Part Affinity Fields}
The approach of Cao et al.\cite{cao2017realtime} inputs a given image into a two-branch convolutional neural network. First a feed forward network simultaneously predicts a set of confidence maps of the body part locations (see \autoref{fig:openPoseProcess} (b)) and a set of vector fields of part affinities, which encode the degree of association between the different parts (see \autoref{fig:openPoseProcess} (c))\cite{cao2017realtime}. After that the confidence maps and the affinity fields are parsed by greedy inference (see \autoref{fig:openPoseProcess}) and they output the 2D key points for all the people in the image\cite{cao2017realtime}.
Like this we could also analyse the shoulder position like it was done in \cite{barathi2016lie} and other movement based emotion detection. However it would not be possible to angle-based evaluations like in McCol et al's approach\cite{mccoll2012affect}. 

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{openPoseProcess.jpg}
\caption{OpenPose process visualized\cite{cao2017realtime}}
\label{fig:openPoseProcess}
\end{figure}

\section{Emotion detection through hand form}
\label{sec:emotionHand}
\subsection{A system for person-independent hand posture recognition against complex backgrounds}
\subsubsection{Hand pose extraction}
Most approaches in this field, require the hand to be in front of a static background or the hand to be the only skin coloured item in the picture, in order for it to be recognized\cite{cui1995learning,hunter1995posture,kohler1997technical,min1999visual}. The approach of Triesch and Malsburg\cite{triesch2001system}, proposes a solution using elastic graph matching (EGM) with multiple feature types. EGM is "a neurally inspired object recognition architecture"\cite{lades1993distortion}. In EGM the views of objects are visualized as a labeled 2-d graph.\cite{triesch2001system}. The nodes of the graph contain a local image description and the edges are labelled with a distance vector which represents the distance between the nodes. 


\subsubsection{Interpretation}
Various graphs are created from training images and then images of new hands are matched to the most similar existing image. In \autoref{egmHand} we can see an example of a graph, which is matched to a new hand.
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{egmHand.jpg}
\caption{Picture of an elastic graph of a hand, and a new hand that matches it\cite{triesch2001system}}
\label{fig:egmHand}
\end{figure}
Like this we can easily recognize if a hand is clenched to a fist ( e.g. anger).

The reported accuracy of this approach is 92.9\% accuracy on simple backgrounds and 85.8\% accuracy on complex background.

\section{Emotion detection through gait}
\label{sec:emotionGait}
\subsection{Emotion Detection from Natural Walking}
\label{subsec:EDNW}
\subsubsection{Gait information extraction}
Although the approach of Cui et al.\cite{cui2016emotion} used is not 100\% fitting to this topic, it is interesting and worth to mention. To extract the data from the gait, they attached two smartphones (Samsung I9100G) to one wrist and one ankle separately\cite{cui2016emotion}.  The phone's accelerometer was used to gather data of the walk. After preprocessing it, various features are extracted using Principal Component Analysis (PCA)\cite{cui2016emotion}.\

To start, the 59 participants were asked to walk naturally back and forth on an area of 6m x 0.8m for two minutes\cite{cui2016emotion}. After that they reported their current emotional state from 1-10\cite{cui2016emotion}. On the first round 1 was no anger and 10 was angry, whereas in the second round 1 was not happy and 10 was very happy.  After that the participants watched an emotional film clip for emotional priming\cite{cui2016emotion}. After this the participants were asked to walk again for one minute, and report the anger score after the walk and recall the anger score right after the film\cite{cui2016emotion}.  After at least three hours, the same experiment was done again with a happy film. 
\subsubsection{Interpretation}
The classification approach was done in different ways. The most successful one was using SVM. It correctly classified the emotion of the walk with ~90\% \cite{cui2016emotion}. 
This approach shows, that we can detect the emotional state of a person from it's gait, however because all people walk very individually, it is necessary to get a baseline walk from each person. Hence it is very hard/ impossible to instantly infer information from a person's walk, when seeing the person for the first time.

\subsection{Emotion recognition using Kinect motion capture data of human gaits}
\label{subsec:LiGait}
\subsubsection{Gait information extraction}
The approach of Li et al.\cite{li2016emotion} uses a set up with two kinect cameras that were placed oppositely at the two ends of a 6m x 1m footpath. The emotional states of the participants were recorded in the same way as in the experiment described under \autoref{subsec:EDNW}. The data reported by the two kinect cameras was processed independently\cite{li2016emotion}. Kinect outputs a stick figure with the different body joints marked (see \autoref{fig:kinectStick}) of the recorded person. The recorded data contains the 3-dimension position of the 14 joints\cite{li2016emotion}. This results in a 42 dimension vector. Because each record consisted of T frames, the data of one record is represented by a T * 42 matrix\cite{li2016emotion}. Fourier transformation was used for further feature extraction\cite{li2016emotion}. 

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{kinectStick.jpg}
\caption{Stick figure extracted by kinect\cite{li2016emotion}}
\label{fig:kinectStick}
\end{figure}

\subsubsection{Interpretation}
The data was classified in various ways using NaiveBayes, RandomForests, LibSVM and SMO\cite{li2016emotion}. The most successful approach was using NaiveBayes.  
The reported accuracy was 80.5\% for recognizing angry and neutral states with the Kinect1 camera, using NaiveBayes. 75\% accuracy was achieved with the Kinect2 camera.
For recognizing happy or neutral, the achieved accuracy was 79.6\% with Kinect1 and 61.8\% with Kinect2. However the distinction between angry and happy only succeeded with an accuracy of 52\%\cite{li2016emotion}.

This approach shows that it is also possible to recognize emotions from human gait, without using any sensors. However it makes a great difference whether the person is recorded from the front or the back.


\section{Proposed model for emotion detection through human gait}
In this section I review the possibilities for recognizing emotions through human gait in a real-time scenario.

\subsection{Possibilities and limitations}
The problem with many emotion recognition algorithms is, that they often need a neutral initialization pose\cite{li2016emotion,cui2016emotion} because humans and especially human gaits are very individual. This makes a real-time system very hard, without having data of a specific person beforehand. However there are various gait databases available\cite{wang2008recognizing,karg2009comparison}, which could be used for training with machine learning algorithms. This still has to be treated with great care because 1. The data bases only consist of acted emotions (currently no spontaneous gait database exists that is publicly acessible) and 2. Due to the high individuality of gaits\cite{karg2012pattern,janssen2008recognition}. In \cite{janssen2008recognition} the inter-individual recognition of emotions through gait was only around chance level.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{interIndividualClassification.jpg}
\caption{Average accuracy of inter individual classification approaches with various pre-processing and classification methods\cite{karg2012pattern}}
\label{fig:interIndividualClassification}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{personDependantClassification.jpg}
\caption{Average accuracy of inter individual classification approaches with various pre-processing and classification methods\cite{karg2012pattern}}
\label{fig:personDependantClassification}
\end{figure}
However this leads to the next problem, the expression of the affective state in gait is often also associated with the velocity\cite{crane2007motion,pollick2001perceiving,roether2009critical}, in addition to the movement itself. This leads to the problem that gait data is often characterized by a high dimensionality, temporal dependency, high variability and nonlinearities\cite{karg2012pattern}, which makes the real-time classification difficult. This is why it is important to preprocess the gathered data and limit the feature space and do a good feature extraction. PCA, KPCA, LDA and GDA are commonly applied for feature selection\cite{karg2012pattern}.\

\autoref{fig:personDependantClassification} and \autoref{fig:interIndividualClassification} highlight the importance of appropriate preprocessing, feature selection and picking a proper classifier as well as the importance of person dependant (\autoref{fig:personDependantClassification}) and person independent classification (\autoref{fig:interIndividualClassification}).\

The emotion detection can be reduced to a classification problem ( classify into one of the 6 emotion types\cite{eckman2003emotions} described in \autoref{sec:emotions}. The most successful approaches for emotion detection in body movement in general (not gait specific) seem to be support vector machines\cite{karg2012pattern,schindler2008recognizing} and artificial neural networks\cite{karg2012pattern, kapur2005gesture}. However there is not much work found for emotion detection through human gait using artificial neural networks\cite{janssen2008recognition}.

\subsection{Concept}
Based on those limitations the following approach could be useful:
\begin{itemize}
\item{Create/ find a large enough gait database to train the model initially}
\item{Extract the pose using openPose\cite{cao2017realtime}, as it is working in real-time and without any special equipment. Any camera can do it.}
\item{Do the feature extraction according to \autoref{fig:interIndividualClassification}. A good choice would be to use Sig. Subsection + LDA\cite{karg2012pattern}}.
\item{Use a k-nn classifier\cite{karg2012pattern}}
\item{Do a continuous recording of the individual's gait, together with an evaluation about how they feel in this moment. This could be easily doable in a workplace environment. If the system can compare person-dependent it will greatly improve it's accuracy}
\end{itemize}

A question which I can not answer is, if the classification can work in real-time, or if it takes too long. All the research found did not do it in real-time. Only the openPose pose extraction algorithm worked in real time \cite{cao2017realtime}.\

An artificial neural networks approach or a combination could also work, but the research is very thin on this topic\cite{janssen2008recognition,karg2012pattern}.



\section{Conclusion}
One thing that we always have to keep in mind is that emotion recognition is not an easy task, also for the human himself\cite{ekman1974detecting,karg2012pattern,schindler2008recognizing}. The approaches in emotion detection through body language are dominated by a range of machine-learning algorithms. The research clearly shows that it is possible to detect human emotions from body language in a fairly accurate way.

There are many different approaches that tackle specific problems in this domain e.g. emotion detection through body position, body movement, gait, hand form etc. Most of them perform fairly well in their specific domain. Sometimes various emotions are harder to distinguish than others. For example distinguishing anger and normal is easier than distinguishing anger and happy\cite{li2016emotion} (see \autoref{subsec:LiGait}) and some emotions share similar features like for example happiness and surprise\cite{kleinsmith2005incremental}.

However a lot of those approaches require "initialisation poses"\cite{li2016emotion,cui2016emotion,mccoll2012affect,singh2015edbl} which does not make the system usable instantly and in real-time. This is not surprising, because humans are distinct individuals and their neutral body poses/ gaits/ movements can be very different from each other.
It is also interesting to see that approaches with neural networks become increasingly more popular. \

We can see that it is not really important to have metrics on how to interpret the different emotions, but instead we can use human-created datasets to train the network and being able to learn how to recognize the emotion.




% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,bibliography}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

%\end{thebibliography}




% that's all folks
\end{document}



